{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for scraping and writing data to a CSV\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base URL\n",
    "base_url = \"https://www.nfl.com/injuries/league/\"\n",
    "\n",
    "# need to loop thru all weeks, and update the base_url with each iteration\n",
    "weeks = [\"REG1\", \"REG2\", \"REG3\", \"REG4\", \"REG5\", \"REG6\", \"REG7\", \"REG8\", \"REG9\", \"REG10\", \"REG11\", \"REG12\", \"REG13\", \"REG14\", \"REG15\", \"REG16\", \"REG17\",\"POST1\", \"POST2\", \"POST3\", \"POST4\"]\n",
    "\n",
    "\n",
    "# Write function to scrape data\n",
    "def scrape_data(url, year):\n",
    "    with open(f'../data/injuries_scrape_{year}.csv', 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # Write the column names\n",
    "        header = [\"Week\", \"Player\", \"Position\", \"Injury\", \"Game Status\", \"Game Type\"] # manually written list\n",
    "        csvwriter.writerow(header)\n",
    "\n",
    "        # Loop thru each week from the 'weeks' list\n",
    "        for week in weeks:\n",
    "            current_url = url+year+'/'+week\n",
    "            response = requests.get(current_url)\n",
    "\n",
    "            if response.status_code == 200: # this is a checkpoint - if status code is 200, all OK, continue\n",
    "                # instantiate BeautifulSoup class\n",
    "                soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "                # create a list of tables from soup object\n",
    "                tables = soup.find_all('table')\n",
    "\n",
    "                # loop thru each table \n",
    "                for table in tables:\n",
    "                    rows = table.find_all('tr') # find the rows in each table\n",
    "\n",
    "                    # loop thru each row\n",
    "                    for row in rows:\n",
    "                        # collect data\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        data = [week] + [col.text.strip() for col in columns]\n",
    "\n",
    "                        # write the row data to csv\n",
    "                        csvwriter.writerow(data)\n",
    "\n",
    "                print(f\"Scraping done for {week}\")\n",
    "                \n",
    "            else: # if status_code != 200, something is wrong\n",
    "                print(f\"Error! Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping done for REG1\n",
      "Scraping done for REG2\n",
      "Scraping done for REG3\n",
      "Scraping done for REG4\n",
      "Scraping done for REG5\n",
      "Scraping done for REG6\n",
      "Scraping done for REG7\n",
      "Scraping done for REG8\n",
      "Scraping done for REG9\n",
      "Scraping done for REG10\n",
      "Scraping done for REG11\n",
      "Scraping done for REG12\n",
      "Scraping done for REG13\n",
      "Scraping done for REG14\n",
      "Scraping done for REG15\n",
      "Scraping done for REG16\n",
      "Scraping done for REG17\n",
      "Scraping done for POST1\n",
      "Scraping done for POST2\n",
      "Scraping done for POST3\n",
      "Scraping done for POST4\n"
     ]
    }
   ],
   "source": [
    "# scrape for 2020\n",
    "scrape_data(base_url, '2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping done for REG1\n",
      "Scraping done for REG2\n",
      "Scraping done for REG3\n",
      "Scraping done for REG4\n",
      "Scraping done for REG5\n",
      "Scraping done for REG6\n",
      "Scraping done for REG7\n",
      "Scraping done for REG8\n",
      "Scraping done for REG9\n",
      "Scraping done for REG10\n",
      "Scraping done for REG11\n",
      "Scraping done for REG12\n",
      "Scraping done for REG13\n",
      "Scraping done for REG14\n",
      "Scraping done for REG15\n",
      "Scraping done for REG16\n",
      "Scraping done for REG17\n",
      "Scraping done for POST1\n",
      "Scraping done for POST2\n",
      "Scraping done for POST3\n",
      "Scraping done for POST4\n"
     ]
    }
   ],
   "source": [
    "# scrape for 2021\n",
    "scrape_data(base_url, '2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the data is scraped - let's clean and combine both data frames\n",
    "\n",
    "# writing a function to clean both dfs\n",
    "\n",
    "def clean_scraped_csvs(year):\n",
    "    \n",
    "    # read csv\n",
    "    df = pd.read_csv(f\"../data/injuries_scrape_{year}.csv\")\n",
    "    \n",
    "    # split 'Week' into --> 'game_type', can merge the 'week' number later on\n",
    "    for i in df['Week']:\n",
    "        if 'REG' in i:\n",
    "            df.loc[df['Week'] == i, 'game_type'] = 'REG'\n",
    "        elif i == 'POST1':\n",
    "            df.loc[df['Week'] == i, 'game_type'] = 'WC'\n",
    "        elif i == 'POST2':\n",
    "            df.loc[df['Week'] == i, 'game_type'] = 'DIV'\n",
    "        elif i == 'POST3':\n",
    "            df.loc[df['Week'] == i, 'game_type'] = 'CON'\n",
    "        elif i == 'POST4':\n",
    "            df.loc[df['Week'] == i, 'game_type'] = 'SB'\n",
    "    \n",
    "    # drop this col now\n",
    "    df.drop('Week', axis = 1, inplace = True)\n",
    "    \n",
    "    # drop these rows --> this part of the scrape pulled headers after every table iteration\n",
    "    df = df[df['Player'] != 'Player']\n",
    "    \n",
    "    # add a season column to ensure the data aligns w/ the proper year\n",
    "    df['season'] = year\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign dfs\n",
    "df_2020 = clean_scraped_csvs('2020')\n",
    "df_2021 = clean_scraped_csvs('2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat both dfs\n",
    "(pd.concat([df_2020, df_2021])).to_csv('../data/cleaned_scraped_data.csv') # NOTE to self: concat funct needs a list to iterate on in order to concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
